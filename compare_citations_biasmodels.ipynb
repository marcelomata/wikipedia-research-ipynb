{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, csv, re\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from IPython.core.debugger import Tracer; debug_here = Tracer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "984\n",
      "984\n"
     ]
    }
   ],
   "source": [
    "# Article bias latest significance test\n",
    "latestcsvpath = '/home/michael/school/cprose_research/wp/wikipedia/data/articles_latest_citebias.csv'\n",
    "cites = [] # citation diff/artlen\n",
    "model = [] # revision bias score/artlen\n",
    "\n",
    "with open(latestcsvpath) as latest:\n",
    "    for row in csv.DictReader(latest):\n",
    "        citeval = float(row['diff_norm'])\n",
    "        modelval = float(row['bias_norm'])\n",
    "        if (float(row['israeli_citations']) > 0 or float(row['palestinian_citations']) > 0) and \\\n",
    "                float(row['total_weight_words']) > 100:\n",
    "            cites.append(citeval)\n",
    "            model.append(modelval)\n",
    "\n",
    "print(len(cites))\n",
    "print(len(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.33171358751207464, 1.0565280237888783e-26)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(cites, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575341\n",
      "575341\n"
     ]
    }
   ],
   "source": [
    "# Article bias all revisions\n",
    "csvpath = '/home/michael/school/cprose_research/wp/wikipedia/data/articles_citebias.csv'\n",
    "cites = [] # citation diff/artlen\n",
    "model = [] # revision bias score/artlen\n",
    "\n",
    "with open(csvpath) as csvfile:\n",
    "    for row in csv.DictReader(csvfile):\n",
    "        citeval = float(row['diff_norm'])\n",
    "        modelval = float(row['bias_norm'])\n",
    "        if (float(row['israeli_citations']) > 0 or float(row['palestinian_citations']) > 0) and \\\n",
    "                float(row['total_weight_words']) > 100:\n",
    "            cites.append(citeval)\n",
    "            model.append(modelval)\n",
    "\n",
    "print(len(cites))\n",
    "print(len(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.31144533791424578, 0.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(cites, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with lengths\n"
     ]
    }
   ],
   "source": [
    "# Create csv for comparing article bias classifications (all articles)\n",
    "artlen_csvpath = '/home/michael/school/cprose_research/wp/wikipedia/data/article_lengths.csv'\n",
    "pages = [line for line in csv.reader(open(artlen_csvpath,'r'))]\n",
    "header = pages[0]\n",
    "pages = pages[1:]\n",
    "pagelens = {}\n",
    "for line in pages:\n",
    "    pagelens[tuple(line[:3])] = int(line[-1])\n",
    "\n",
    "print(\"Done with lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2032\n",
      "2032\n"
     ]
    }
   ],
   "source": [
    "# Create csv for comparing article bias classifications (just latest articles)\n",
    "citations_csvpath = '/home/michael/school/cprose_research/wp/wikipedia/data/citations_reverts.csv'\n",
    "pages = [line for line in csv.reader(open(citations_csvpath,'r'))]\n",
    "header = pages[0]\n",
    "pages = pages[1:]\n",
    "pagecites = {}\n",
    "for line in pages:\n",
    "    #pagecites[line[0]] = [float(el) for el in line[4:7]] # Gets latest revisions\n",
    "    #pagecites[line[0]] = line # Gets latest revisions\n",
    "    isrcite = int(line[4])\n",
    "    palcite = int(line[5])\n",
    "    diff = isrcite - palcite\n",
    "    total = sum(int(el) for el in line[4:7])\n",
    "    isrcite_norm = (0 if total==0 else float(isrcite)/total)\n",
    "    palcite_norm = (0 if total==0 else float(palcite)/total)\n",
    "    diff_norm = (0 if total==0 else float(diff)/total)\n",
    "    pagecites[line[0]] = [el for el in line[:2]] + [float(el) for el in line[4:7]] + \\\n",
    "                            [diff, total, isrcite_norm, palcite_norm, diff_norm]\n",
    "\n",
    "article_bias_csvpath = '/home/michael/school/cprose_research/wp/wikipedia/data/ipc_article_biases.csv'\n",
    "biases = [line for line in csv.reader(open(article_bias_csvpath,'r'))]\n",
    "biasheader = biases[0]\n",
    "biases = biases[1:]\n",
    "pagebiases = {}\n",
    "for b in biases:\n",
    "    pagebiases[b[0]] = [float(el) for el in b[-6:-2]]\n",
    "    \n",
    "print(len(pagecites))\n",
    "print(len(pagebiases))\n",
    "\n",
    "csv_outpath = '/home/michael/school/cprose_research/wp/wikipedia/data/articles_latest_citebias.csv'\n",
    "with open(csv_outpath, 'w') as out:\n",
    "    writer = csv.writer(out)\n",
    "    writer.writerow(header[:2] + header[4:7] + ['diff','total_cite','isrcite_norm','palcite_norm','diff_norm'] + \\\n",
    "                     biasheader[4:8])\n",
    "    for art in sorted(pagecites.keys()):\n",
    "        writer.writerow(pagecites[art] + pagebiases[art])\n",
    "        #debug_here()\n",
    "        \n",
    "csv_outpath = '/home/michael/school/cprose_research/wp/wikipedia/data/articles_citebias.csv'\n",
    "with open(csv_outpath, 'w') as out:\n",
    "    writer = csv.writer(out)\n",
    "    writer.writerow(header[:2] + header[4:7] + ['diff','total_cite','isrcite_norm','palcite_norm','diff_norm'] + \\\n",
    "                    ['art_length'] + biasheader[4:8] + [b+'_norm' for b in biasheader[4:8]])\n",
    "    for art in sorted(pagecites.keys()):\n",
    "        if art in pagebiases.keys():\n",
    "            writer.writerow(pagecites[art] + [pagelens[art]] + list(pagebiases[art]))\n",
    "        #debug_here()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with lengths\n",
      "Done with citations\n",
      "1089176\n",
      "1089171\n"
     ]
    }
   ],
   "source": [
    "#%debug\n",
    "# Create csv for comparing article bias classifications (all articles)\n",
    "artlen_csvpath = '/home/michael/school/cprose_research/wp/wikipedia/data/article_lengths.csv'\n",
    "pages = [line for line in csv.reader(open(artlen_csvpath,'r'))]\n",
    "header = pages[0]\n",
    "pages = pages[1:]\n",
    "pagelens = {}\n",
    "for line in pages:\n",
    "    pagelens[tuple(line[:3])] = int(line[-1])\n",
    "\n",
    "print(\"Done with lengths\")\n",
    "\n",
    "citations_csvpath = '/home/michael/school/cprose_research/wp/wikipedia/data/citations_reverts.csv'\n",
    "pages = [line for line in csv.reader(open(citations_csvpath,'r'))]\n",
    "header = pages[0]\n",
    "pages = pages[1:]\n",
    "pagecites = {}\n",
    "for line in pages:\n",
    "    #pagecites[line[0]] = [float(el) for el in line[4:7]] # Gets latest revisions\n",
    "    #pagecites[line[0]] = line # Gets latest revisions\n",
    "    isrcite = int(line[4])\n",
    "    palcite = int(line[5])\n",
    "    diff = isrcite - palcite\n",
    "    total = isrcite+palcite+int(line[6])\n",
    "    artlen = pagelens[tuple(line[:3])]\n",
    "    isrcite_norm = (0 if total==0 else float(isrcite)/artlen) # Doesn't make sense\n",
    "    palcite_norm = (0 if total==0 else float(palcite)/artlen)\n",
    "    diff_norm = (0 if total==0 else float(diff)/artlen)\n",
    "    pagecites[tuple(line[:3])] = [el for el in line[:2]] + [float(el) for el in line[4:7]] + \\\n",
    "                            [diff, total, isrcite_norm, palcite_norm, diff_norm]\n",
    "\n",
    "print(\"Done with citations\")\n",
    "\n",
    "article_bias_csvpath = '/home/michael/school/cprose_research/wp/wikipedia/data/ipc_article_biases.csv'\n",
    "biases = [line for line in csv.reader(open(article_bias_csvpath,'r'))]\n",
    "biasheader = biases[0]\n",
    "biases = biases[1:]\n",
    "pagebiases = {}\n",
    "for b in biases:\n",
    "    artlen = pagelens[tuple(b[:3])]\n",
    "    pagebiases[tuple(b[:3])] = [float(el) for el in b[-6:-2]] + [float(el)/artlen for el in b[-6:-2]]\n",
    "    \n",
    "print(len(pagecites))\n",
    "print(len(pagebiases))\n",
    "\n",
    "#%debug\n",
    "csv_outpath = '/home/michael/school/cprose_research/wp/wikipedia/data/articles_citebias.csv'\n",
    "with open(csv_outpath, 'w') as out:\n",
    "    writer = csv.writer(out)\n",
    "    writer.writerow(header[:2] + header[4:7] + ['diff','total_cite','isrcite_norm','palcite_norm','diff_norm'] + \\\n",
    "                    ['art_length'] + biasheader[4:8] + [b+'_norm' for b in biasheader[4:8]])\n",
    "    for art in sorted(pagecites.keys()):\n",
    "        if art in pagebiases.keys():\n",
    "            writer.writerow(pagecites[art] + [pagelens[art]] + list(pagebiases[art]))\n",
    "        #debug_here()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204992"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Editor csv\n",
    "citationbias = dict(json.loads(open(\"/home/michael/school/cprose_research/wp/wikipedia/data/editor_citationbias.json\",'r').read()))\n",
    "len(citationbias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0.0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citationbias[list(citationbias.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def biases(csvpath):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            username: the user whose edits should be saved (for qualitative checking)\n",
    "    \"\"\"\n",
    "    model_biases = defaultdict(float)\n",
    "    numedits = defaultdict(int)\n",
    "    \n",
    "    with open(csvpath, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader) # Skip first row\n",
    "        prev_artname = \"\"\n",
    "        prev_bias = 0\n",
    "        for row in reader:\n",
    "            art = row[0]\n",
    "            username = row[2]\n",
    "            numedits[username] += 1\n",
    "            comment = row[3]\n",
    "            bias = float(row[4])\n",
    "            if not 'rvv' in comment and not 'vandal' in comment:\n",
    "                if art == prev_artname:\n",
    "                    bias_change = prev_bias - bias\n",
    "                    model_biases[username] += bias_change\n",
    "                else:\n",
    "                    model_biases[username] += bias\n",
    "\n",
    "            prev_artname = art\n",
    "            prev_bias = bias\n",
    "            \n",
    "    return model_biases, numedits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def useredits(csvpath, selected):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            username: the user whose edits should be saved (for qualitative checking)\n",
    "    \"\"\"\n",
    "    useredits = []\n",
    "    \n",
    "    with open(csvpath, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader) # Skip first row\n",
    "        prev_artname = \"\"\n",
    "        prev_bias = 0\n",
    "        for row in reader:\n",
    "            art = row[0]\n",
    "            username = row[2]\n",
    "            comment = row[3]\n",
    "            bias = float(row[4])\n",
    "            if not 'rvv' in comment and not 'vandal' in comment:\n",
    "                if art == prev_artname:\n",
    "                    bias_change = prev_bias - bias\n",
    "                if username == selected:\n",
    "                    useredits.append(row + [bias_change])\n",
    "\n",
    "            prev_artname = art\n",
    "            prev_bias = bias\n",
    "            \n",
    "    if useredits:\n",
    "        useredits = sorted(useredits, key=itemgetter(-1))\n",
    "    \n",
    "    return useredits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "205362"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build model biases dictionary\n",
    "csvpath = '/home/michael/school/cprose_research/wp/wikipedia/data/ipc_article_biases.csv'\n",
    "\n",
    "modelbiases, numedits = biases(csvpath)\n",
    "            \n",
    "print(len(list(modelbiases.keys())))\n",
    "len(numedits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441\n",
      "872\n",
      "1308\n",
      "1477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1479"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get bot list\n",
    "bots = []\n",
    "cnum = \"\"\n",
    "done = False\n",
    "\n",
    "# Get bot category page\n",
    "category_base_url = \"https://en.wikipedia.org/w/api.php?\" + \\\n",
    "    \"action=query&\" +  \\\n",
    "    \"list=categorymembers&\" + \\\n",
    "    \"format=json&\" + \\\n",
    "    \"cmtitle=Category%3A{:s}&\" +  \\\n",
    "    \"cmprop=title&\" + \\\n",
    "    \"cmlimit=500\" + \\\n",
    "    \"&cmcontinue={:s}\"\n",
    "    \n",
    "while not done:\n",
    "    category_url = category_base_url.format(urllib.parse.quote(\"All_Wikipedia_bots\"), cnum)\n",
    "    #print(category_url)\n",
    "\n",
    "    page =  urllib.request.urlopen(category_url).read().decode('utf-8')\n",
    "    #print(page)\n",
    "\n",
    "    # Extract bot names\n",
    "    botsp = re.findall(r'\"User\\:(?P<username>[\\w\\ /\\.]*)\"\\}', page)\n",
    "    botsp = [re.sub(r'/.*', '', ed) for ed in botsp]\n",
    "    #print(botsp[:10])\n",
    "    bots += botsp\n",
    "    print(len(bots))\n",
    "\n",
    "    # Get continue number\n",
    "    #cnum = re.search(r'cmcontinue\":\"(?P<cnum>[^\"]*)\"', page)\n",
    "    cnum = re.search(r'cmcontinue\":\"([^\"]*)\"', page)\n",
    "    if cnum:\n",
    "        cnum = cnum.group(1)\n",
    "        #print(cnum)\n",
    "    else:\n",
    "        done = True\n",
    "\n",
    "# Add other bots\n",
    "other_bots = ['SmackBot', 'Tawkerbot2']\n",
    "bots += other_bots\n",
    "len(bots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get revision lengths\n",
    "artlen_csvpath = '/home/michael/school/cprose_research/wp/wikipedia/data/article_lengths.csv'\n",
    "pages = [line for line in csv.reader(open(artlen_csvpath,'r'))]\n",
    "header = pages[0]\n",
    "pages = pages[1:]\n",
    "pagelens = {}\n",
    "for line in pages:\n",
    "    pagelens[tuple(line[:3])] = int(line[-1])\n",
    "\n",
    "print(\"Done with lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3612"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagelens[('Shabbat', '2008-12-24 16:21:45', 'Glane23')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3242"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagelens[('Tzipi Livni', '2009-12-21 16:18:53', '82.1.54.219')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Peter Deer', -25343.121521899342),\n",
      " ('Jayjg', -18635.96141006936),\n",
      " ('ResearchFinder', -18206.084239807526),\n",
      " ('Tawkerbot4', -11948.824870282524),\n",
      " ('Curps', -11132.559114150774),\n",
      " ('Cureden', -10992.294565723589),\n",
      " ('Wiki alf', -9344.224974998313),\n",
      " ('82.1.54.219', -8569.240751205996),\n",
      " ('Cometstyles', -7980.491574382942),\n",
      " ('El C', -6935.012873889626)]\n",
      "[('Jon513', 7983.616634151831),\n",
      " ('Glane23', 8289.253614389705),\n",
      " ('Antonio Lopez', 10728.471832793477),\n",
      " ('Ixfd64', 14495.444826636338),\n",
      " ('212.98.136.42', 16586.595097739915),\n",
      " ('Pan Dan', 18217.451190746408),\n",
      " ('HistoryBuffEr', 21005.42305323439),\n",
      " ('Sjakkalle', 35590.582086142625),\n",
      " ('Mike Rosoft', 197060.75086122236),\n",
      " ('Yamamoto Ichiro', 198915.87273713664)]\n"
     ]
    }
   ],
   "source": [
    "# Check sum_bias top editors\n",
    "sorted_modelbias = sorted([item for item in modelbiases.items() \n",
    "                           if not re.match(r'^[?\\ �]+$',item[0]) and numedits[item[0]] > 5 and \\\n",
    "                              item[0] not in bots], key=itemgetter(1))\n",
    "pprint(sorted_modelbias[:10])\n",
    "pprint(sorted_modelbias[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Jews',\n",
       "  '2005-11-03 03:09:25',\n",
       "  'Ixfd64',\n",
       "  'Reverted edits by [[Special:Contributions/193.62.43.210|193.62.43.210]] to last version by Ixfd64',\n",
       "  '293.58786455966424',\n",
       "  '3147.2750787444716',\n",
       "  '-952.70814877236853',\n",
       "  '1486.57807581262',\n",
       "  '1',\n",
       "  '1',\n",
       "  7.5735113701366],\n",
       " ['Jews',\n",
       "  '2005-09-23 21:29:37',\n",
       "  'Ixfd64',\n",
       "  'Reverted edits by [[Special:Contributions/Jtkiefer|Jtkiefer]] to last version by 203.221.152.225',\n",
       "  '-0.06750087992651628',\n",
       "  '2.9999999999999991',\n",
       "  '-0.76861850051644254',\n",
       "  '1.8999747228493362',\n",
       "  '1',\n",
       "  '1',\n",
       "  285.09827532518244],\n",
       " ['Jews',\n",
       "  '2006-06-30 15:39:13',\n",
       "  'Ixfd64',\n",
       "  'Reverted edits by [[Special:Contributions/66.61.54.253|66.61.54.253]] ([[User talk:66.61.54.253|talk]]) to last version by Njaard',\n",
       "  '332.6212056077128',\n",
       "  '3713.4267031472164',\n",
       "  '-1147.0089153824742',\n",
       "  '1756.9420277445647',\n",
       "  '1',\n",
       "  '1',\n",
       "  14958.139856889391]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%debug\n",
    "# Get most distinguishing edits for a certain editor\n",
    "edits = useredits(csvpath, 'Ixfd64')\n",
    "#edits[:3]\n",
    "edits[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "editorbiases = {}\n",
    "    if b[2] not in editorbiases:\n",
    "    editorbiases[b[2]] = []\n",
    "    if b[0] in pagebiases:\n",
    "    editorbiases[b[2]].append([float(el)-prv for (el,prv) in zip(b[-6:-2],pagebiases[b[0]])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

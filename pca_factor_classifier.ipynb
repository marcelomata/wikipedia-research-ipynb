{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from random import random\n",
    "\n",
    "from IPython.core.debugger import Tracer; debug_here = Tracer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CLASSIFYING 1 DOCUMENT/EDITOR, with cross-validation\n",
    "# Prepare data\n",
    "threshold = 0.2\n",
    "data = pd.read_csv('/home/michael/school/research/wp/wikipedia/data/editortext_factors_{:1.1f}.csv'.format(threshold))\n",
    "eds = {}\n",
    "\n",
    "# Get neutral editors\n",
    "rows = data[data['factor']==0]\n",
    "eds[0] = set(rows['editor'].tolist())\n",
    "\n",
    "edx = {}\n",
    "edy = {}\n",
    "\n",
    "for i in range(1, 11):\n",
    "    rows = data[data['factor']==i]\n",
    "    eds[i] = set(rows['editor'].tolist())\n",
    "    \n",
    "    # Split up editor corpus (including neutral editors)\n",
    "    edx[i] = list(eds[0]) + list(eds[i])\n",
    "    edy[i] = [0] * len(eds[0]) + [1] * len(eds[i])\n",
    "    \n",
    "# Extract features\n",
    "# vectorizer = CountVectorizer(stop_words='english', min_df=1)\n",
    "# vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "corpus = {}\n",
    "X = {}\n",
    "y = {}\n",
    "\n",
    "# Fit to factor words\n",
    "for i in range(1, 11):\n",
    "#     print(i)\n",
    "    corpus[i] = []\n",
    "    y[i] = []\n",
    "    \n",
    "    # Get text from editors (includes neutral editors)\n",
    "    for j, ed in enumerate(edx[i]):\n",
    "        text = data[data['editor']==ed]['additions'].values[0]\n",
    "        if isinstance(text, str):\n",
    "            corpus[i].append(text)\n",
    "            y[i].append(edy[i][j])\n",
    "        \n",
    "    vectorizer.fit(corpus[i])\n",
    "        \n",
    "# Fit to neutral words\n",
    "# vectorizer.fit([c for c in data[data['editor'].isin(eds[0])]['additions'].tolist() if not isinstance(c, float)])\n",
    "\n",
    "# for i in range(1, 11):\n",
    "    X[i] = vectorizer.transform(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CLASSIFYING 1 DOCUMENT/EDITOR, with cross-validation--CITATION WINDOW\n",
    "# Prepare data\n",
    "threshold = 0.2\n",
    "data = pd.read_csv('/home/michael/school/research/wp/wp_articles/editor_citewindow_factor{:1.1f}.csv'.format(threshold))\n",
    "eds = {}\n",
    "\n",
    "# Get neutral editors\n",
    "rows = data[data['factor']==0]\n",
    "eds[0] = set(rows['editor'].tolist())\n",
    "\n",
    "edx = {}\n",
    "edy = {}\n",
    "\n",
    "for i in range(1, 11):\n",
    "    rows = data[data['factor']==i]\n",
    "    eds[i] = set(rows['editor'].tolist())\n",
    "    \n",
    "    # Split up editor corpus (including neutral editors)\n",
    "    edx[i] = list(eds[0]) + list(eds[i])\n",
    "    edy[i] = [0] * len(eds[0]) + [1] * len(eds[i])\n",
    "    \n",
    "# Extract features\n",
    "# vectorizer = CountVectorizer(stop_words='english', min_df=1)\n",
    "# vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "corpus = {}\n",
    "X = {}\n",
    "y = {}\n",
    "\n",
    "# Fit to factor words\n",
    "for i in range(1, 11):\n",
    "#     print(i)\n",
    "    corpus[i] = []\n",
    "    y[i] = []\n",
    "    \n",
    "    # Get text from editors (includes neutral editors)\n",
    "    for j, ed in enumerate(edx[i]):\n",
    "        text = data[data['editor']==ed]['text'].values[0]\n",
    "        if isinstance(text, str):\n",
    "            corpus[i].append(text)\n",
    "            y[i].append(edy[i][j])\n",
    "        \n",
    "    vectorizer.fit(corpus[i])\n",
    "        \n",
    "# Fit to neutral words\n",
    "# vectorizer.fit([c for c in data[data['editor'].isin(eds[0])]['additions'].tolist() if not isinstance(c, float)])\n",
    "\n",
    "# for i in range(1, 11):\n",
    "    X[i] = vectorizer.transform(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 461\n",
      "1 204\n",
      "2 123\n",
      "3 128\n",
      "4 46\n",
      "5 162\n",
      "6 94\n",
      "7 215\n",
      "8 126\n",
      "9 100\n",
      "10 190\n"
     ]
    }
   ],
   "source": [
    "# Print number of editors\n",
    "for i in range(11):\n",
    "    print(i, len(eds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: \taccuracy:\t0.693270 (+/- 0.01)\n",
      "\tall neutral:\t0.693233\n",
      "\trandom:\t\t0.574678\n",
      "\n",
      "2: \taccuracy:\t0.789438 (+/- 0.01)\n",
      "\tall neutral:\t0.789384\n",
      "\trandom:\t\t0.667486\n",
      "\n",
      "3: \taccuracy:\t0.782717 (+/- 0.01)\n",
      "\tall neutral:\t0.782683\n",
      "\trandom:\t\t0.659819\n",
      "\n",
      "4: \taccuracy:\t0.909365 (+/- 0.02)\n",
      "\tall neutral:\t0.909270\n",
      "\trandom:\t\t0.835004\n",
      "\n",
      "5: \taccuracy:\t0.740002 (+/- 0.01)\n",
      "\tall neutral:\t0.739968\n",
      "\trandom:\t\t0.615169\n",
      "\n",
      "6: \taccuracy:\t0.830703 (+/- 0.01)\n",
      "\tall neutral:\t0.830631\n",
      "\trandom:\t\t0.718633\n",
      "\n",
      "7: \taccuracy:\t0.681988 (+/- 0.01)\n",
      "\tall neutral:\t0.681953\n",
      "\trandom:\t\t0.566214\n",
      "\n",
      "8: \taccuracy:\t0.785405 (+/- 0.01)\n",
      "\tall neutral:\t0.785349\n",
      "\trandom:\t\t0.662848\n",
      "\n",
      "9: \taccuracy:\t0.821742 (+/- 0.00)\n",
      "\tall neutral:\t0.821747\n",
      "\trandom:\t\t0.707042\n",
      "\n",
      "10: \taccuracy:\t0.708135 (+/- 0.00)\n",
      "\tall neutral:\t0.708141\n",
      "\trandom:\t\t0.586646\n",
      "\n",
      "\n",
      "mean accuracy:\t\t 0.774276399442\n",
      "mean all neutral:\t 0.774235799968\n",
      "mean random:\t\t 0.659353882057\n"
     ]
    }
   ],
   "source": [
    "# Train classifier -- CITATION WINDOW\n",
    "clf = MultinomialNB()\n",
    "results = []\n",
    "zeror = []\n",
    "rands = []\n",
    "kappa_scorer = make_scorer(cohen_kappa_score)\n",
    "kappas = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    print(\"{:d}: \".format(i), end='')\n",
    "\n",
    "    # accuracy\n",
    "    scores = cross_validation.cross_val_score(clf, X[i], y[i], cv=10)\n",
    "    results.append(scores.mean())\n",
    "    print(\"\\taccuracy:\\t%f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    # kappa\n",
    "#     scores = cross_validation.cross_val_score(clf, X[i], y[i], cv=10, scoring=kappa_scorer)\n",
    "#     kappa = scores.mean()\n",
    "#     kappas.append(kappa)\n",
    "#     print(\"\\tkappa:\\t\\t\".format(kappa))\n",
    "\n",
    "    # Random baseline\n",
    "    factor_perc = len(eds[i])/(len(eds[i]) + len(eds[0]))\n",
    "    neutral_perc = 1-factor_perc\n",
    "    zeror.append(neutral_perc)\n",
    "    print('\\tall neutral:\\t{:f}'.format(neutral_perc))\n",
    "    \n",
    "    rand_baseline = factor_perc**2 + neutral_perc**2\n",
    "    rands.append(rand_baseline)\n",
    "    print('\\trandom:\\t\\t{:f}'.format(rand_baseline))\n",
    "    print()\n",
    "    \n",
    "print()\n",
    "print('mean accuracy:\\t\\t', np.mean(results))\n",
    "print('mean all neutral:\\t', np.mean(zeror))\n",
    "# print('mean kappa:\\t', np.mean(kappas))\n",
    "print('mean random:\\t\\t', np.mean(rands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: \taccuracy:\t0.707421 (+/- 0.01)\n",
      "\tall neutral:\t0.707801\n",
      "\trandom:\t\t0.586363\n",
      "\n",
      "2: \taccuracy:\t0.798137 (+/- 0.01)\n",
      "\tall neutral:\t0.797125\n",
      "\trandom:\t\t0.676566\n",
      "\n",
      "3: \taccuracy:\t0.794281 (+/- 0.01)\n",
      "\tall neutral:\t0.789557\n",
      "\trandom:\t\t0.667686\n",
      "\n",
      "4: \taccuracy:\t0.917229 (+/- 0.02)\n",
      "\tall neutral:\t0.915596\n",
      "\trandom:\t\t0.845441\n",
      "\n",
      "5: \taccuracy:\t0.752307 (+/- 0.01)\n",
      "\tall neutral:\t0.752640\n",
      "\trandom:\t\t0.627653\n",
      "\n",
      "6: \taccuracy:\t0.839869 (+/- 0.01)\n",
      "\tall neutral:\t0.840067\n",
      "\trandom:\t\t0.731292\n",
      "\n",
      "7: \taccuracy:\t0.696534 (+/- 0.01)\n",
      "\tall neutral:\t0.695955\n",
      "\trandom:\t\t0.576797\n",
      "\n",
      "8: \taccuracy:\t0.796856 (+/- 0.01)\n",
      "\tall neutral:\t0.797125\n",
      "\trandom:\t\t0.676566\n",
      "\n",
      "9: \taccuracy:\t0.825938 (+/- 0.01)\n",
      "\tall neutral:\t0.826159\n",
      "\trandom:\t\t0.712759\n",
      "\n",
      "10: \taccuracy:\t0.715546 (+/- 0.01)\n",
      "\tall neutral:\t0.715925\n",
      "\trandom:\t\t0.593248\n",
      "\n",
      "\n",
      "mean accuracy:\t\t 0.784411807903\n",
      "mean all neutral:\t 0.783795047397\n",
      "mean random:\t\t 0.66943709778\n"
     ]
    }
   ],
   "source": [
    "# Train classifier\n",
    "clf = MultinomialNB()\n",
    "results = []\n",
    "zeror = []\n",
    "rands = []\n",
    "kappa_scorer = make_scorer(cohen_kappa_score)\n",
    "kappas = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    print(\"{:d}: \".format(i), end='')\n",
    "\n",
    "    # accuracy\n",
    "    scores = cross_validation.cross_val_score(clf, X[i], y[i], cv=10)\n",
    "    results.append(scores.mean())\n",
    "    print(\"\\taccuracy:\\t%f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    # kappa\n",
    "#     scores = cross_validation.cross_val_score(clf, X[i], y[i], cv=10, scoring=kappa_scorer)\n",
    "#     kappa = scores.mean()\n",
    "#     kappas.append(kappa)\n",
    "#     print(\"\\tkappa:\\t\\t\".format(kappa))\n",
    "\n",
    "    # Random baseline\n",
    "    factor_perc = len(eds[i])/(len(eds[i]) + len(eds[0]))\n",
    "    neutral_perc = 1-factor_perc\n",
    "    zeror.append(neutral_perc)\n",
    "    print('\\tall neutral:\\t{:f}'.format(neutral_perc))\n",
    "    \n",
    "    rand_baseline = factor_perc**2 + neutral_perc**2\n",
    "    rands.append(rand_baseline)\n",
    "    print('\\trandom:\\t\\t{:f}'.format(rand_baseline))\n",
    "    print()\n",
    "    \n",
    "print()\n",
    "print('mean accuracy:\\t\\t', np.mean(results))\n",
    "print('mean all neutral:\\t', np.mean(zeror))\n",
    "# print('mean kappa:\\t', np.mean(kappas))\n",
    "print('mean random:\\t\\t', np.mean(rands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: \taccuracy:\t0.522676 (+/- 0.12)\n",
      "\tall neutral:\t0.707801\n",
      "\trandom:\t\t0.586363\n",
      "\n",
      "2: \taccuracy:\t0.543218 (+/- 0.15)\n",
      "\tall neutral:\t0.797125\n",
      "\trandom:\t\t0.676566\n",
      "\n",
      "3: \taccuracy:\t0.537380 (+/- 0.13)\n",
      "\tall neutral:\t0.789557\n",
      "\trandom:\t\t0.667686\n",
      "\n",
      "4: \taccuracy:\t0.778514 (+/- 0.14)\n",
      "\tall neutral:\t0.915596\n",
      "\trandom:\t\t0.845441\n",
      "\n",
      "5: \taccuracy:\t0.572115 (+/- 0.15)\n",
      "\tall neutral:\t0.752640\n",
      "\trandom:\t\t0.627653\n",
      "\n",
      "6: \taccuracy:\t0.585519 (+/- 0.12)\n",
      "\tall neutral:\t0.840067\n",
      "\trandom:\t\t0.731292\n",
      "\n",
      "7: \taccuracy:\t0.510524 (+/- 0.13)\n",
      "\tall neutral:\t0.695955\n",
      "\trandom:\t\t0.576797\n",
      "\n",
      "8: \taccuracy:\t0.538990 (+/- 0.11)\n",
      "\tall neutral:\t0.797125\n",
      "\trandom:\t\t0.676566\n",
      "\n",
      "9: \taccuracy:\t0.580639 (+/- 0.15)\n",
      "\tall neutral:\t0.826159\n",
      "\trandom:\t\t0.712759\n",
      "\n",
      "10: \taccuracy:\t0.525966 (+/- 0.13)\n",
      "\tall neutral:\t0.715925\n",
      "\trandom:\t\t0.593248\n",
      "\n",
      "\n",
      "mean accuracy:\t\t 0.569554135712\n",
      "mean all neutral:\t 0.783795047397\n",
      "mean accuracy:\t\t 0.66943709778\n"
     ]
    }
   ],
   "source": [
    "# Train classifier\n",
    "clf = MultinomialNB()\n",
    "results = []\n",
    "zeror = []\n",
    "rands = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    print(\"{:d}: \".format(i), end='')\n",
    "\n",
    "    # Test\n",
    "    scores = cross_validation.cross_val_score(clf, X[i], y[i], cv=10)\n",
    "    results.append(scores.mean())\n",
    "    print(\"\\taccuracy:\\t%f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "    # Random baseline\n",
    "    factor_perc = len(eds[i])/(len(eds[i]) + len(eds[0]))\n",
    "    neutral_perc = 1-factor_perc\n",
    "    zeror.append(neutral_perc)\n",
    "    print('\\tall neutral:\\t{:f}'.format(neutral_perc))\n",
    "    \n",
    "    rand_baseline = factor_perc**2 + neutral_perc**2\n",
    "    rands.append(rand_baseline)\n",
    "    print('\\trandom:\\t\\t{:f}'.format(rand_baseline))\n",
    "    print()\n",
    "    \n",
    "print()\n",
    "print('mean accuracy:\\t\\t', np.mean(results))\n",
    "print('mean all neutral:\\t', np.mean(zeror))\n",
    "print('mean accuracy:\\t\\t', np.mean(rands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.683547461644\n",
      "\trandom:\t\t 0.500311837346\n",
      "\tall factor:\t 0.644006486217\n",
      "\tall neutral:\t 0.355993513783\n",
      "2: 0.702021653732\n",
      "\trandom:\t\t 0.49765258216\n",
      "\tall factor:\t 0.750407205136\n",
      "\tall neutral:\t 0.249592794864\n",
      "3: 0.565072830906\n",
      "\trandom:\t\t 0.503008233059\n",
      "\tall factor:\t 0.496200126662\n",
      "\tall neutral:\t 0.503799873338\n",
      "4: 0.557328853423\n",
      "\trandom:\t\t 0.4996100078\n",
      "\tall factor:\t 0.843043139137\n",
      "\tall neutral:\t 0.156956860863\n",
      "5: 0.611668897813\n",
      "\trandom:\t\t 0.5\n",
      "\tall factor:\t 0.697568049978\n",
      "\tall neutral:\t 0.302431950022\n",
      "6: 0.536787564767\n",
      "\trandom:\t\t 0.502849740933\n",
      "\tall factor:\t 0.673056994819\n",
      "\tall neutral:\t 0.326943005181\n",
      "7: 0.712840888568\n",
      "\trandom:\t\t 0.498555174282\n",
      "\tall factor:\t 0.720787430016\n",
      "\tall neutral:\t 0.279212569984\n",
      "8: 0.579863739591\n",
      "\trandom:\t\t 0.488872066616\n",
      "\tall factor:\t 0.536866010598\n",
      "\tall neutral:\t 0.463133989402\n",
      "9: 0.578982009653\n",
      "\trandom:\t\t 0.485300570426\n",
      "\tall factor:\t 0.568451075033\n",
      "\tall neutral:\t 0.431548924967\n",
      "10: 0.533207157605\n",
      "\trandom:\t\t 0.509463179628\n",
      "\tall factor:\t 0.508602890571\n",
      "\tall neutral:\t 0.491397109429\n",
      "\n",
      "0.60613210577\n"
     ]
    }
   ],
   "source": [
    "# Prepare corpus -- editor-aware\n",
    "data = pd.read_csv('/home/michael/school/research/wp/wikipedia/data/article_diffs_factors_0.2.csv')\n",
    "# data = pd.read_csv('/home/michael/school/research/wp/wikipedia/data/article_diffs_factors_0.6.csv')\n",
    "eds = {}\n",
    "\n",
    "# Get neutral editors\n",
    "rows = data[data['factor']==0]\n",
    "eds[0] = set(rows['editor'].tolist())\n",
    "\n",
    "edx_train = {}\n",
    "edx_test = {}\n",
    "edy_train = {}\n",
    "edy_test = {}\n",
    "\n",
    "for i in range(1, 11):\n",
    "    rows = data[data['factor']==i]\n",
    "    eds[i] = set(rows['editor'].tolist())\n",
    "    \n",
    "    # Split up editor corpus (including neutral editors)\n",
    "    X = list(eds[0]) + list(eds[i])\n",
    "    y = [0] * len(eds[0]) + [1] * len(eds[i])\n",
    "    edx_train[i], edx_test[i], edy_train[i], edy_test[i] = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "# Extract features\n",
    "# vectorizer = CountVectorizer(stop_words='english', min_df=1)\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "# vectorizer = TfidfVectorizer(min_df=1)\n",
    "corpus_train = {}\n",
    "corpus_test = {}\n",
    "X_train = {}\n",
    "X_test = {}\n",
    "y_train = {}\n",
    "y_test = {}\n",
    "\n",
    "# Fit to factor words\n",
    "for i in range(1, 11):\n",
    "#     print(i)\n",
    "    corpus_train[i] = []\n",
    "    corpus_test[i] = []\n",
    "    y_train[i] = []\n",
    "    y_test[i] = []\n",
    "    \n",
    "    # Get text from editors (includes neutral editors)\n",
    "    for j, ed in enumerate(edx_train[i]):\n",
    "        tgt = edy_train[i][j]\n",
    "        contributions = []\n",
    "        for k, row in data[data['editor']==ed].iterrows():\n",
    "            if not isinstance(row['additions'], float):\n",
    "                contributions.append(str(row['additions']))\n",
    "        corpus_train[i] += contributions\n",
    "        y_train[i] += [tgt] * len(contributions)\n",
    "        \n",
    "    vectorizer.fit(corpus_train[i])\n",
    "        \n",
    "    for j, ed in enumerate(edx_test[i]):\n",
    "        tgt = edy_test[i][j]\n",
    "        contributions = []\n",
    "        for k, row in data[data['editor']==ed].iterrows():\n",
    "            if not isinstance(row['additions'], float):\n",
    "                contributions.append(row['additions'])\n",
    "        corpus_test[i] += contributions\n",
    "        y_test[i] += [tgt] * len(contributions)\n",
    "        \n",
    "    vectorizer.fit(corpus_test[i])\n",
    "    \n",
    "# Fit to neutral words\n",
    "vectorizer.fit([c for c in data[data['editor'].isin(eds[0])]['additions'].tolist() if not isinstance(c, float)])\n",
    "\n",
    "for i in range(1, 11):\n",
    "    X_train[i] = vectorizer.transform(corpus_train[i])\n",
    "    X_test[i] = vectorizer.transform(corpus_test[i])\n",
    "#     print(X_train[i].size)\n",
    "\n",
    "# Train classifier\n",
    "clf = MultinomialNB()\n",
    "results = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    print(\"{:d}: \".format(i), end='')\n",
    "    clf.fit(X_train[i], y_train[i])\n",
    "\n",
    "    # Test\n",
    "    predicted = clf.predict(X_test[i])\n",
    "    result = np.mean(predicted == y_test[i])\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "\n",
    "    # Random baseline\n",
    "    baseline = np.array([round(random()) for _ in range(len(y_test[i]))])\n",
    "    print('\\trandom:\\t\\t', np.mean(baseline == y_test[i]))\n",
    "    baseline = np.array([1 for _ in range(len(y_test[i]))]) # predict always the factor\n",
    "    print('\\tall factor:\\t', np.mean(baseline == y_test[i]))\n",
    "    baseline = np.array([0 for _ in range(len(y_test[i]))]) # predict always neutral\n",
    "    print('\\tall neutral:\\t', np.mean(baseline == y_test[i]))\n",
    "    \n",
    "print()\n",
    "print(np.mean(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of editors:\n",
      "\t0 499\n",
      "\t1: 206\n",
      "\t2: 126\n",
      "\t3: 131\n",
      "\t4: 46\n",
      "\t5: 164\n",
      "\t6: 95\n",
      "\t7: 217\n",
      "\t8: 127\n",
      "\t9: 105\n",
      "\t10: 198\n",
      "\n",
      "1: 0.69240364226\n",
      "\trandom:\t\t 0.501559186728\n",
      "\tall factor:\t 0.644006486217\n",
      "\tall neutral:\t 0.355993513783\n",
      "2: 0.704608604005\n",
      "\trandom:\t\t 0.49640701351\n",
      "\tall factor:\t 0.750407205136\n",
      "\tall neutral:\t 0.249592794864\n",
      "3: 0.56380620646\n",
      "\trandom:\t\t 0.49778340722\n",
      "\tall factor:\t 0.496200126662\n",
      "\tall neutral:\t 0.503799873338\n",
      "4: 0.551328973421\n",
      "\trandom:\t\t 0.507469850603\n",
      "\tall factor:\t 0.843043139137\n",
      "\tall neutral:\t 0.156956860863\n",
      "5: 0.602855867916\n",
      "\trandom:\t\t 0.491633199465\n",
      "\tall factor:\t 0.697568049978\n",
      "\tall neutral:\t 0.302431950022\n",
      "6: 0.54481865285\n",
      "\trandom:\t\t 0.501295336788\n",
      "\tall factor:\t 0.673056994819\n",
      "\tall neutral:\t 0.326943005181\n",
      "7: 0.714646920715\n",
      "\trandom:\t\t 0.501625428933\n",
      "\tall factor:\t 0.720787430016\n",
      "\tall neutral:\t 0.279212569984\n",
      "8: 0.580923542771\n",
      "\trandom:\t\t 0.506131718395\n",
      "\tall factor:\t 0.536866010598\n",
      "\tall neutral:\t 0.463133989402\n",
      "9: 0.575252303642\n",
      "\trandom:\t\t 0.507459412023\n",
      "\tall factor:\t 0.568451075033\n",
      "\tall neutral:\t 0.431548924967\n",
      "10: 0.523399862354\n",
      "\trandom:\t\t 0.491913282863\n",
      "\tall factor:\t 0.508602890571\n",
      "\tall neutral:\t 0.491397109429\n",
      "\n",
      "0.605404457639\n"
     ]
    }
   ],
   "source": [
    "# ORIGINAL--faster/different than above\n",
    "# Prepare corpus -- editor-aware (editors divided train/test)\n",
    "data = pd.read_csv('/home/michael/school/research/wp/wikipedia/data/article_diffs_factors_0.2.csv')\n",
    "# data = pd.read_csv('/home/michael/school/research/wp/wikipedia/data/article_diffs_factors_0.6.csv')\n",
    "eds = {}\n",
    "\n",
    "# Get neutral editors\n",
    "rows = data[data['factor']==0]\n",
    "eds[0] = set(rows['editor'].tolist())\n",
    "print(\"Number of editors:\")\n",
    "print(\"\\t0:\", len(eds[0]))\n",
    "\n",
    "edx_train = {}\n",
    "edx_test = {}\n",
    "edy_train = {}\n",
    "edy_test = {}\n",
    "\n",
    "for i in range(1, 11):\n",
    "    rows = data[data['factor']==i]\n",
    "    eds[i] = set(rows['editor'].tolist())\n",
    "    \n",
    "    # Split up editors\n",
    "    X = list(eds[0]) + list(eds[i])\n",
    "    print(\"\\t{0}: {1}\".format(i, len(eds[i])))\n",
    "    y = [0] * len(eds[0]) + [1] * len(eds[i])\n",
    "    edx_train[i], edx_test[i], edy_train[i], edy_test[i] = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features\n",
    "# vectorizer = CountVectorizer(stop_words='english', min_df=1)\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "# vectorizer = TfidfVectorizer(min_df=1)\n",
    "corpus_train = {}\n",
    "corpus_test = {}\n",
    "X_train = {}\n",
    "X_test = {}\n",
    "y_train = {}\n",
    "y_test = {}\n",
    "\n",
    "# Fit to neutral words\n",
    "vectorizer.fit([c for c in data[data['editor'].isin(eds[0])]['additions'].tolist() if not isinstance(c, float)])\n",
    "\n",
    "for i in range(1, 11):\n",
    "    corpus_train[i] = []\n",
    "    corpus_test[i] = []\n",
    "    y_train[i] = []\n",
    "    y_test[i] = []\n",
    "    \n",
    "    # Get text from editors\n",
    "    for j, ed in enumerate(edx_train[i]):\n",
    "        contributions = data[data['editor']==ed]['additions'].tolist()\n",
    "        contributions = [c for c in contributions if not isinstance(c, float)]\n",
    "        corpus_train[i] += contributions\n",
    "        y_train[i] += [edy_train[i][j]] * len(contributions)\n",
    "        \n",
    "    vectorizer.fit(corpus_train[i])\n",
    "        \n",
    "    for j, ed in enumerate(edx_test[i]):\n",
    "        contributions = [c for c in data[data['editor']==ed]['additions'].tolist() if not isinstance(c, float)]\n",
    "        corpus_test[i] += contributions\n",
    "        y_test[i] += [edy_test[i][j]] * len(contributions)\n",
    "        \n",
    "    vectorizer.fit(corpus_test[i])\n",
    "    \n",
    "for i in range(1, 11):\n",
    "    X_train[i] = vectorizer.transform(corpus_train[i])\n",
    "    X_test[i] = vectorizer.transform(corpus_test[i])\n",
    "#     print(X_train[i].size)\n",
    "\n",
    "# Train classifier\n",
    "clf = MultinomialNB()\n",
    "results = []\n",
    "print()\n",
    "\n",
    "for i in range(1,11):\n",
    "    print(\"{:d}: \".format(i), end='')\n",
    "    clf.fit(X_train[i], y_train[i])\n",
    "\n",
    "    # Test\n",
    "    predicted = clf.predict(X_test[i])\n",
    "    result = np.mean(predicted == y_test[i])\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "\n",
    "    # Random baseline\n",
    "    baseline = np.array([round(random()) for _ in range(len(y_test[i]))])\n",
    "    print('\\trandom:\\t\\t', np.mean(baseline == y_test[i]))\n",
    "    baseline = np.array([1 for _ in range(len(y_test[i]))]) # predict always the factor\n",
    "    print('\\tall factor:\\t', np.mean(baseline == y_test[i]))\n",
    "    baseline = np.array([0 for _ in range(len(y_test[i]))]) # predict always neutral\n",
    "    print('\\tall neutral:\\t', np.mean(baseline == y_test[i]))\n",
    "    \n",
    "print()\n",
    "print(np.mean(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

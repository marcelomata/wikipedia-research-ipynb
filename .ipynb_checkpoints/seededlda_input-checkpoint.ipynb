{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare stopwords\n",
    "custom_puncstops = [\"''\", \"``\", \"--\"]\n",
    "puncstops = [c for c in string.punctuation]\n",
    "puncstops.extend(custom_puncstops)\n",
    "\n",
    "custom_stops = ['template', \"'s\", 'ref', '/ref', 'url=http', 'http', 'name=', 'c', 'harvnb']\n",
    "\n",
    "stops = stopwords.words('english')[:]\n",
    "stops.extend(puncstops)\n",
    "stops.extend(custom_stops)\n",
    "len(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2067"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and prepare the full Wikipedia latest edition corpus\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "corpus_csvpath = '/home/michael/school/cprose_research/wp/wikipedia/data/ipc_articles_latest.tsv'\n",
    "wp_docs = []\n",
    "with open(corpus_csvpath) as corpus_csv:\n",
    "    reader = csv.reader(corpus_csv, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    #headings = next(reader)\n",
    "    for line in reader:\n",
    "        text = line[2]\n",
    "        \n",
    "        # Tokenize\n",
    "        doctoks = nltk.word_tokenize(text)\n",
    "        \n",
    "        # Make lowercase\n",
    "        lowtoks = [tok.lower() for tok in doctoks]\n",
    "        \n",
    "        # Remove punctuation and stopwords\n",
    "        cleantoks = [tok for tok in lowtoks if not tok in stops]\n",
    "        \n",
    "        wp_docs.append(cleantoks)\n",
    "\n",
    "len(wp_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283052"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test full dictionary\n",
    "\n",
    "dictpath = '/home/michael/school/cprose_research/seededlda/cl2_project/SeededLDA/data/wp/wp_wordlist.txt'\n",
    "\n",
    "alltoks = sum(wp_docs, [])\n",
    "vocab = set(alltoks)\n",
    "print(len(alltoks))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trim down dataset for testing\n",
    "wptrim = [wp_docs[i] for i in range(0, 2066, 10)]\n",
    "len(wptrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344879\n",
      "50990\n"
     ]
    }
   ],
   "source": [
    "# Write trimmed dictionary\n",
    "\n",
    "dictpath = '/home/michael/school/cprose_research/seededlda/cl2_project/SeededLDA/data/wp/wptrim_wordlist.txt'\n",
    "\n",
    "alltoks = sum(wptrim, [])\n",
    "vocab = set(alltoks)\n",
    "vocablist = list(vocab)\n",
    "print(len(alltoks))\n",
    "print(len(vocab))\n",
    "\n",
    "with open(dictpath, 'w') as out:\n",
    "    for wd in vocablist:\n",
    "        out.write(wd+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0k\n",
      "10k\n",
      "20k\n",
      "30k\n",
      "40k\n",
      "50k\n",
      "60k\n",
      "70k\n",
      "80k\n",
      "90k\n",
      "100k\n",
      "110k\n",
      "120k\n",
      "130k\n",
      "140k\n",
      "150k\n",
      "160k\n",
      "170k\n",
      "180k\n",
      "190k\n",
      "200k\n",
      "210k\n",
      "220k\n",
      "230k\n",
      "240k\n",
      "250k\n",
      "260k\n",
      "270k\n",
      "280k\n",
      "290k\n",
      "300k\n",
      "310k\n",
      "320k\n",
      "330k\n",
      "340k\n"
     ]
    }
   ],
   "source": [
    "# Write trimmed wd indices\n",
    "wdind_path = '/home/michael/school/cprose_research/seededlda/cl2_project/SeededLDA/data/wp/wptrim_wordi.txt'\n",
    "\n",
    "with open(wdind_path, 'w') as out:\n",
    "    for i, tok in enumerate(alltoks):\n",
    "        out.write(str(vocablist.index(tok)) + '\\n')\n",
    "        if i % 10000 == 0:\n",
    "            print(\"{:d}k\".format(int(i/1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# Write trimmed doc indices\n",
    "doci_path = '/home/michael/school/cprose_research/seededlda/cl2_project/SeededLDA/data/wp/wptrim_doci.txt'\n",
    "\n",
    "with open(doci_path, 'w') as out:\n",
    "    for i, doc in enumerate(wptrim):\n",
    "        for tok in doc:\n",
    "            out.write(str(i) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
